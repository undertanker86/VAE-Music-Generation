{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchaudio\nimport torchaudio.transforms as T\nimport numpy as np\nimport librosa\nimport librosa.display\nimport IPython.display as ipd\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport json\nimport timm\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom torch.utils.data import Dataset, DataLoader\nfrom IPython.display import Audio\nfrom tqdm import tqdm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:06:49.906582Z","iopub.execute_input":"2025-05-18T08:06:49.907309Z","iopub.status.idle":"2025-05-18T08:07:01.862462Z","shell.execute_reply.started":"2025-05-18T08:06:49.907274Z","shell.execute_reply":"2025-05-18T08:07:01.861735Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!gdown 1PzzqApEdJXHmOMCO4HMyLN7v7O8368cW\n!unzip -q results.zip -d results\nos.remove(\"results.zip\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:07:01.863778Z","iopub.execute_input":"2025-05-18T08:07:01.864124Z","iopub.status.idle":"2025-05-18T08:07:59.000668Z","shell.execute_reply.started":"2025-05-18T08:07:01.864105Z","shell.execute_reply":"2025-05-18T08:07:58.999631Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_and_get_genres(json_path):\n    with open(json_path, \"r\") as f:\n        data = json.load(f)\n    return data.get('genres', [])\n\njson_dir = os.path.join(\"results\", \"crawled_data\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:07:59.002038Z","iopub.execute_input":"2025-05-18T08:07:59.002285Z","iopub.status.idle":"2025-05-18T08:07:59.007321Z","shell.execute_reply.started":"2025-05-18T08:07:59.002259Z","shell.execute_reply":"2025-05-18T08:07:59.006557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_and_resample_audio(file_path, target_sr=22050):\n    audio, sr = librosa.load(file_path, sr=None)\n    if sr != target_sr:\n        audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n    return audio, target_sr\n\ndef audio_to_melspec(audio, sr, n_mels, n_fft=2048, hop_length=512, to_db=False):\n    spec = librosa.feature.melspectrogram(\n        y=audio,\n        sr=sr,\n        n_fft=n_fft,\n        hop_length=hop_length,\n        win_length=None,\n        window=\"hann\",\n        center=True,\n        pad_mode=\"reflect\",\n        power=2.0,\n        n_mels=n_mels\n    )\n\n    if to_db:\n        spec = librosa.power_to_db(spec, ref=np.max)\n\n    return spec\n    \ndef normalize_melspec(melspec, norm_range=(0, 1)):\n    scaler = MinMaxScaler(feature_range=norm_range)\n    melspec = melspec.T\n    melspec_normalized = scaler.fit_transform(melspec)\n    return melspec_normalized.T\n\ndef denormalize_melspec(melspec_normalized, original_melspec, norm_range=(0, 1)):\n    scaler = MinMaxScaler(feature_range=norm_range)\n    melspec = original_melspec.T\n    scaler.fit(melspec)\n    melspec_denormalized = scaler.inverse_transform(melspec_normalized.T)\n    return melspec_denormalized.T\n    \ndef melspec_to_audio(melspec, sr, n_fft=2048, hop_length=512, n_iter=64):\n    if np.any(melspec < 0):\n        melspec = librosa.db_to_power(melspec)\n\n    audio_reconstructed = librosa.feature.inverse.mel_to_audio(\n        melspec,\n        sr=sr,\n        n_fft=n_fft,\n        hop_length=hop_length,\n        win_length=None,\n        window=\"hann\",\n        center=True,\n        pad_mode=\"reflect\",\n        power=2.0,\n        n_iter=n_iter\n    )\n\n    return audio_reconstructed\n\n\ndef display_audio_files(reconstructed_audio, sr, title=\"\", original_audio=None):\n    if original_audio is not None:\n        print(\"Original Audio:\")\n        ipd.display(ipd.Audio(original_audio, rate=sr))\n        print(\"Reconstructed Audio (from Mel Spectrogram):\")\n    else:\n        print(title)\n\n    ipd.display(ipd.Audio(reconstructed_audio, rate=sr))\n\ndef show_spectrogram(spectrogram, title=\"Mel-Spectrogram\", denormalize=False, is_numpy=False):\n    if not is_numpy:\n        spectrogram = spectrogram.squeeze().cpu().numpy()\n    plt.figure(figsize=(10, 4))\n    if denormalize:\n        plt.imshow(spectrogram, aspect=\"auto\", origin=\"lower\", cmap=\"viridis\")\n    else:\n        plt.imshow(spectrogram, aspect=\"auto\", origin=\"lower\", cmap=\"viridis\", vmin=0, vmax=1)\n    plt.title(title)\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Mel Frequency\")\n    plt.colorbar()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:07:59.009240Z","iopub.execute_input":"2025-05-18T08:07:59.009459Z","iopub.status.idle":"2025-05-18T08:07:59.026099Z","shell.execute_reply.started":"2025-05-18T08:07:59.009443Z","shell.execute_reply":"2025-05-18T08:07:59.025500Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"json_dir = os.path.join(\"results\", \"crawled_data\")\nall_genres = []\n\nfor filename in os.listdir(json_dir):\n    if filename.endswith('.json'):\n        json_path = os.path.join(json_dir, filename)\n        genres = load_and_get_genres(json_path)\n        all_genres.extend(genres)\n\nunique_genres = set(all_genres)\nmax_genres = len(unique_genres)\nprint(f\"Total unique genres: {max_genres}\")\nprint(f\"Unique genres: {unique_genres}\")\n\ngenres2idx = {genre: idx for idx, genre in enumerate(unique_genres)}\nidx2genres = {idx: genre for genre, idx in genres2idx.items()}\n\ndef tokenize(genres):\n    return [genres2idx[genre] for genre in genres if genre in genres2idx]\n\ndef detokenize_tolist(tokens):\n    return [idx2genres[token] for token in tokens if token in idx2genres]\n\ndef onehot_encode(tokens, max_genres):\n    onehot = np.zeros(max_genres)\n    onehot[tokens] = 1\n    return onehot\n\ndef onehot_decode(onehot):\n    return [idx for idx, val in enumerate(onehot) if val == 1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:07:59.026796Z","iopub.execute_input":"2025-05-18T08:07:59.027061Z","iopub.status.idle":"2025-05-18T08:07:59.055930Z","shell.execute_reply.started":"2025-05-18T08:07:59.027044Z","shell.execute_reply":"2025-05-18T08:07:59.055384Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create PyTorch DataLoader","metadata":{}},{"cell_type":"code","source":"class AudioDataset(Dataset):\n    def __init__(self, data_dir, json_dir, sample_rate, duration, n_mels, n_genres, testset_amount=10):\n        self.data_dir = data_dir\n        self.files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(\".mp3\")]\n        self.json_dir = json_dir\n        self.json_files = [os.path.join(json_dir, f) for f in os.listdir(json_dir) if f.endswith(\".json\")]\n        self.sample_rate = sample_rate\n        self.duration = duration\n        self.fixed_length = sample_rate * duration\n        self.n_genres = n_genres\n        self.n_mels = n_mels\n\n        audios = []\n        for file_path, json_file_path in tqdm(zip(self.files, self.json_files), desc=f\"Loading audio files in {data_dir}\", unit=\"file\", total=len(self.files)):\n            audio, sr = load_and_resample_audio(file_path, target_sr=sample_rate)\n            genres_list = load_and_get_genres(json_file_path)\n\n            genres_tokens = tokenize(genres_list)\n            genres_input = onehot_encode(genres_tokens, n_genres)\n            genres_input = torch.tensor(genres_input, dtype=torch.long).unsqueeze(0)\n\n            n_samples = len(audio)\n            n_segments = n_samples // self.fixed_length\n\n            for i in range(n_segments):\n                start = i * self.fixed_length\n                end = (i + 1) * self.fixed_length\n                segment = audio[start:end]\n                mel_spec = audio_to_melspec(segment, sr, self.n_mels, to_db=True)\n                mel_spec_norm = normalize_melspec(mel_spec)\n                mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0)\n                mel_spec_norm = torch.tensor(mel_spec_norm, dtype=torch.float32).unsqueeze(0)\n                audios.append((mel_spec_norm, genres_input, mel_spec))\n\n        self.audios = audios[:len(audios) - testset_amount]\n        self.testset = audios[len(audios) - testset_amount:]\n        print(f\"Loaded {len(self.audios)} audio segments from {len(self.files)} files, each with shape: {self.audios[0][0].shape}, {self.audios[0][1].shape}, duration: {duration} seconds\")\n        print(f\"Test set: {len(self.testset)} audio segments\")\n\n\n    def __len__(self):\n        return len(self.audios)\n\n    def __getitem__(self, idx):\n        mel_spec_part, genres_input, mel_spec = self.audios[idx]\n\n        return mel_spec_part, genres_input, mel_spec","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:07:59.056638Z","iopub.execute_input":"2025-05-18T08:07:59.057010Z","iopub.status.idle":"2025-05-18T08:07:59.065507Z","shell.execute_reply.started":"2025-05-18T08:07:59.056984Z","shell.execute_reply":"2025-05-18T08:07:59.064852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_rate = 22050\nduration = 3\nn_mels = 256\n\naudio_dir = os.path.join(\"results\", \"crawled_data\", \"audio\")\njson_dir = os.path.join(\"results\", \"crawled_data\")\n\ntestset_amount = 32\ntrainset = AudioDataset(audio_dir, json_dir, sample_rate, duration,\n                        n_mels, max_genres, testset_amount=testset_amount)\ntestset = trainset.testset\n\nif len(trainset) == 0:\n    raise ValueError(f\"No .wav file found in {audio_dir}.\")\n\ntrainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=4)\ntestloader = DataLoader(testset, batch_size=testset_amount, shuffle=False, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:07:59.066316Z","iopub.execute_input":"2025-05-18T08:07:59.066527Z","iopub.status.idle":"2025-05-18T08:13:06.947508Z","shell.execute_reply.started":"2025-05-18T08:07:59.066512Z","shell.execute_reply":"2025-05-18T08:13:06.946765Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"audios = trainloader.dataset.audios.copy()\n\nframe = audios[0][0].shape[-1]\nprint(audios[0][1])\nprint(frame)\n\nfor i in range(1):\n    show_spectrogram(audios[i][0], title=\"Clip Mel-Spectrogram\")\n    spec_denorm = denormalize_melspec(audios[i][0].numpy().squeeze(), audios[i][2].numpy().squeeze())\n    show_spectrogram(torch.tensor(spec_denorm), title=\"Denormalized Mel-Spectrogram\", denormalize=True)\n    audio_reconstructed = melspec_to_audio(spec_denorm, sample_rate)\n    display_audio_files(audio_reconstructed, sample_rate, title=\"Original Audio after convert to Spectrogram and back to Audio\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:13:06.948252Z","iopub.execute_input":"2025-05-18T08:13:06.948755Z","iopub.status.idle":"2025-05-18T08:13:09.540326Z","shell.execute_reply.started":"2025-05-18T08:13:06.948726Z","shell.execute_reply":"2025-05-18T08:13:09.539633Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nclass CVAE(nn.Module):\n    def __init__(self, d_model, latent_dim, n_frames, n_mels, n_genres):\n        super(CVAE, self).__init__()\n        self.d_model = d_model\n        self.latent_dim = latent_dim\n        self.n_frames = int(np.ceil(n_frames / (2**3)))\n        self.n_mels = int(np.ceil(n_mels / (2**3)))\n        self.n_genres = n_genres\n        print(f\"Downsampled dimensions: {self.n_frames} x {self.n_mels}\")\n\n        # Encoder layers with proper tracking for skip connections\n        self.encoder_conv1 = nn.Conv2d(1 + self.n_genres, d_model, kernel_size=3, stride=2, padding=1)\n        self.encoder_bn1 = nn.BatchNorm2d(d_model)\n        self.encoder_act1 = nn.SiLU()\n        self.encoder_drop1 = nn.Dropout2d(0.05)\n\n        self.encoder_conv2 = nn.Conv2d(d_model, d_model * 2, kernel_size=3, stride=2, padding=1)\n        self.encoder_bn2 = nn.BatchNorm2d(d_model * 2)\n        self.encoder_act2 = nn.SiLU()\n        self.encoder_drop2 = nn.Dropout2d(0.1)\n\n        self.encoder_conv3 = nn.Conv2d(d_model * 2, d_model * 4, kernel_size=3, stride=2, padding=1)\n        self.encoder_bn3 = nn.BatchNorm2d(d_model * 4)\n        self.encoder_act3 = nn.SiLU()\n        self.encoder_drop3 = nn.Dropout2d(0.15)\n\n        self.encoder_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.encoder_flatten = nn.Flatten()\n\n        # Latent space\n        self.fc_mu = nn.Linear(d_model * 4, latent_dim)\n        self.fc_logvar = nn.Linear(d_model * 4, latent_dim)\n\n        # Decoder\n        self.decoder_input = nn.Linear(latent_dim + self.n_genres, d_model * 4 * self.n_frames * self.n_mels)\n        \n        # Decoder layers with proper channel dimensions for skip connections\n        self.decoder_conv1 = nn.ConvTranspose2d(d_model * 4, d_model * 2, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.decoder_bn1 = nn.BatchNorm2d(d_model * 2)\n        self.decoder_act1 = nn.SiLU()\n        self.decoder_drop1 = nn.Dropout2d(0.1)\n\n        # Note: Skip connection will add d_model*2 channels, so next layer input is d_model*4\n        self.decoder_conv2 = nn.ConvTranspose2d(d_model * 4, d_model, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.decoder_bn2 = nn.BatchNorm2d(d_model)\n        self.decoder_act2 = nn.SiLU()\n        self.decoder_drop2 = nn.Dropout2d(0.05)\n\n        # Note: Skip connection will add d_model channels, so next layer input is d_model*2\n        self.decoder_conv3 = nn.ConvTranspose2d(d_model * 2, 1, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.decoder_sigmoid = nn.Sigmoid()\n\n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, x, genres_input):\n        ori_genres_embed = genres_input.view(genres_input.size(0), -1)\n        genres_embed = ori_genres_embed.unsqueeze(-1).unsqueeze(-1)\n        genres_embed = genres_embed.expand(-1, -1, x.size(2), x.size(3))\n        x_genres = torch.cat((x, genres_embed), dim=1)\n\n        # Encoder pass with skip connection collection\n        h = x_genres\n        \n        # Layer 1\n        h = self.encoder_conv1(h)\n        h = self.encoder_bn1(h)\n        h = self.encoder_act1(h)\n        skip1 = h  # Save skip connection before dropout\n        h = self.encoder_drop1(h)\n        \n        # Layer 2  \n        h = self.encoder_conv2(h)\n        h = self.encoder_bn2(h)\n        h = self.encoder_act2(h)\n        skip2 = h  # Save skip connection before dropout\n        h = self.encoder_drop2(h)\n        \n        # Layer 3\n        h = self.encoder_conv3(h)\n        h = self.encoder_bn3(h)\n        h = self.encoder_act3(h)\n        h = self.encoder_drop3(h)\n        \n        # Final pooling and flattening\n        h = self.encoder_pool(h)\n        h = self.encoder_flatten(h)\n\n        # Latent space\n        mu = self.fc_mu(h)\n        logvar = self.fc_logvar(h)\n        z = self.reparameterize(mu, logvar)\n        z_genres = torch.cat((z, ori_genres_embed), dim=1)\n\n        # Decoder\n        h_dec = self.decoder_input(z_genres)\n        h_dec = h_dec.view(-1, self.d_model * 4, self.n_frames, self.n_mels)\n\n        # Decoder layer 1\n        h_dec = self.decoder_conv1(h_dec)\n        h_dec = self.decoder_bn1(h_dec)\n        h_dec = self.decoder_act1(h_dec)\n        h_dec = self.decoder_drop1(h_dec)\n        \n        # Add skip connection from encoder layer 2 (matching channels: d_model*2)\n        if h_dec.shape[2:] != skip2.shape[2:]:\n            # Interpolate to match spatial dimensions\n            skip2_resized = F.interpolate(skip2, size=h_dec.shape[2:], mode='bilinear', align_corners=False)\n        else:\n            skip2_resized = skip2\n        h_dec = torch.cat([h_dec, skip2_resized], dim=1)  # Concatenate instead of add\n        \n        # Decoder layer 2\n        h_dec = self.decoder_conv2(h_dec)\n        h_dec = self.decoder_bn2(h_dec)\n        h_dec = self.decoder_act2(h_dec)\n        h_dec = self.decoder_drop2(h_dec)\n        \n        # Add skip connection from encoder layer 1 (matching channels: d_model)\n        if h_dec.shape[2:] != skip1.shape[2:]:\n            # Interpolate to match spatial dimensions\n            skip1_resized = F.interpolate(skip1, size=h_dec.shape[2:], mode='bilinear', align_corners=False)\n        else:\n            skip1_resized = skip1\n        h_dec = torch.cat([h_dec, skip1_resized], dim=1)  # Concatenate instead of add\n        \n        # Final decoder layer\n        h_dec = self.decoder_conv3(h_dec)\n        h_dec = self.decoder_sigmoid(h_dec)\n\n        # Crop/pad to match original input size\n        recon = h_dec[:, :, :x.size(2), :x.size(3)]\n        return recon, mu, logvar","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:16:00.219027Z","iopub.execute_input":"2025-05-18T08:16:00.219855Z","iopub.status.idle":"2025-05-18T08:16:00.236737Z","shell.execute_reply.started":"2025-05-18T08:16:00.219823Z","shell.execute_reply":"2025-05-18T08:16:00.236008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef loss_function(recon_x, x, mu, logvar):\n    recon_loss = F.mse_loss(recon_x, x, reduction=\"sum\")\n    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return recon_loss + KLD","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:16:05.641838Z","iopub.execute_input":"2025-05-18T08:16:05.642545Z","iopub.status.idle":"2025-05-18T08:16:05.646857Z","shell.execute_reply.started":"2025-05-18T08:16:05.642523Z","shell.execute_reply":"2025-05-18T08:16:05.645787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_vae(model, dataloader, optimizer, scheduler, num_epochs, verbose_interval=50):\n    model.train()\n    losses = []\n    for epoch in tqdm(range(num_epochs), desc=\"Training\", unit=\"epoch\"):\n        train_loss = 0\n        for batch_idx, (data, genres_input, ori_data) in enumerate(dataloader):\n            data = data.to(device)\n            genres_input = genres_input.to(device)\n\n            optimizer.zero_grad()\n\n            recon, mu, logvar = model(data, genres_input)\n            loss = loss_function(recon, data, mu, logvar)\n            loss.backward()\n            train_loss += loss.item()\n            optimizer.step()\n\n        scheduler.step()\n        avg_loss = train_loss / len(dataloader.dataset)\n        losses.append(avg_loss)\n        print(f\"Epoch {epoch}/{num_epochs}, Loss: {avg_loss:.4f}, Lr: {scheduler.get_last_lr()[0]}\")\n        if epoch == 0 or (epoch + 1) % verbose_interval == 0:\n            data = data[0].detach().cpu()\n            recon_img = recon[0].detach().cpu()\n            show_spectrogram(data, title=\"Original Spectrogram\")\n            show_spectrogram(recon_img, title=\"Reconstructed Spectrogram\")\n    return mu, logvar, losses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:16:06.950941Z","iopub.execute_input":"2025-05-18T08:16:06.951194Z","iopub.status.idle":"2025-05-18T08:16:06.957400Z","shell.execute_reply.started":"2025-05-18T08:16:06.951174Z","shell.execute_reply":"2025-05-18T08:16:06.956735Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"d_model = 64\nlatent_dim = 128\nlr = 2e-4\nnum_epochs = 100\nstep_size = num_epochs//2\nverbose_interval = num_epochs//10\ngamma = 0.5\n\nmodel = CVAE(d_model, latent_dim, n_mels, frame, max_genres).to(device)\noptimizer = optim.AdamW(model.parameters(), lr=lr)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n\nprint(f\"Total number of parameters: {sum(p.numel() for p in model.parameters())}\")\nmu, logvar, losses = train_vae(model, trainloader, optimizer, scheduler, num_epochs, verbose_interval=verbose_interval)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:16:33.253287Z","iopub.execute_input":"2025-05-18T08:16:33.253896Z","execution_failed":"2025-05-18T08:14:33.868Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), \"model_checkpoint.pth\")\n\nmodel = CVAE(d_model, latent_dim, n_mels, frame, max_genres).to(device)\nmodel.load_state_dict(torch.load(\"model_checkpoint.pth\"))\nmodel.eval()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-18T08:14:33.869Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_losses(losses, title=\"Training Loss\", xlabel=\"Epochs\", ylabel=\"Loss\", color='b', grid=True):\n    plt.figure(figsize=(10, 6))\n    plt.plot(losses, color=color, linewidth=2)\n    plt.title(title, fontsize=16, fontweight=\"bold\")\n    plt.xlabel(xlabel, fontsize=14)\n    plt.ylabel(ylabel, fontsize=14)\n    plt.xticks(fontsize=12)\n    plt.yticks(fontsize=12)\n    plt.grid(grid, linestyle=\"--\", alpha=0.6)\n\n    min_loss_idx = losses.index(min(losses))\n    max_loss_idx = losses.index(max(losses))\n\n    plt.annotate(f\"Min Loss: {min(losses):.4f}\",\n                 xy=(min_loss_idx, min(losses)),\n                 xytext=(min_loss_idx + 1, min(losses) + 0.1),\n                 arrowprops=dict(arrowstyle=\"->\", color=\"green\"),\n                 fontsize=12, color='green')\n\n    plt.annotate(f\"Max Loss: {max(losses):.4f}\",\n                 xy=(max_loss_idx, max(losses)),\n                 xytext=(max_loss_idx + 1, max(losses) + 0.1),\n                 arrowprops=dict(arrowstyle=\"->\", color=\"red\"),\n                 fontsize=12, color=\"red\")\n\n    plt.tight_layout()\n    plt.show()\n\nplot_losses(losses)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-18T08:14:33.869Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}